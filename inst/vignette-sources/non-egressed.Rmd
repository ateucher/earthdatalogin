---
title: "Send code to the data? Non-egress S3 Access"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Mosaic images using STAC}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


A commonly repeated but _often misguided_ quip about cloud-based workflows is 

> send the compute to the data, not the data to the compute.

It may seem intuitive that upload a few kb of code is better than downloading terrabytes of data.
However, this logic can easily be deeply misleading, even if it is lucratively profitable for cloud providers.

If you are fortunate enough to care nothing about costs or have someone else covering those costs, the technical logic of this statement is accurate, though even then not in the way it is often understood.
The rest of us must consider the financial cost of renting compute from a cloud provider far outweighs charges for data egress.
But wait! Isn't it just unfeasible to download terrabytes of data?
That is where this quip is particularly misleading.
Cloud native workflows access data using _range requests_ rather than downloading the entire dataset to the disks of a local filesystem -- that is, we never really 'download' entire files, but instead 'stream' only the parts of each file we are actively using --
just as we consume Netflix by 'streaming' the data to our local computers rather than waiting for a download that must be stored on a harddisk.
Anyone with a network connection capable of streaming video can also stream research data.

What about all the computing power? Many users of compute instances from cloud providers are in fact renting virtual machines with less computational power than the laptops or desktops (even cell phones) they already own. Moreover, well-designed cloud-native data workflows frequently avoid onerous demands on disk speed, memory, or even cpu threads.  The limiting resource in most cloud-native workflows is network bandwidth.
If you have a network connection capable of streaming high-definition video, you probably already have a setup that will allow cloud-based computing.  
Even if you don't, small virtual machines are offered in a free tier by most major cloud providers. 
GitHub Codespaces is particularly easy environment for anyone to get started using.
Free-tier machines may have limited resources -- only a few cores, a few GB of RAM and limited disk space -- but they almost always have high bandwidth network with speeds easily above 100 Mb/s range -- ideal for cloud-native workflows.  

This is not to say that it never makes sense to 'send the code to the data'.
As network bandwidth is often rate-limiting, anyone in search of the best performance will naturally want to seek out the fastest network connection to the data -- the local area network of the specific regional data center housing the data. Inside the data center, the network is the fastest and most reliable (dropped packets or timeouts can be significant issues elsewhere). 
Even small compute inside the center can have impressive performance, and it will be the compute you can afford rather than the network speed that can hold you back.

Pricing reflects this reality. Inside the data center, there are no bandwidth charges for accessing the data, because data never 'egresses' from the data center over the internet. 
In contrast, sending large amounts of data over public internet networks is not free to Amazon or the other cloud providers, and so they pass these charges onto their customers as egress rates (around $0.02/GB in `us-west-2`). Amazon is happy to levy these charges against either the requester or the provider of the data (and may waive the charges in the case of some public datasets -- of course as a consumer it is not possible to distinguish this from a provider-pays contract).  

NASA EarthData has taken a somewhat novel approach to this situation. To allow users to access these publicly funded data resources without paying Amazon for the privilege, NASA has created an EarthDataLogin system that routes public https requests to these data products through a system of redirects in a CloudFlare content distribution network. This routing requires users to register and provide an authentication token, both which are freely available through the EarthDataLogin platform and API.



If users are willing to pay Amazon for compute inside `us-west-2`, they can take advantage
of extremely fast network without paying egress charges (since no data egress occurs.)
Perhaps surprisingly, this internal access still requires authentication to generate AWS access tokens
(an id, secret token, and a session token).  These tokens will only work from compute inside the AWS data center -- trying to use these tokens from other machines will throw an Access Denied error. 
More inconveniently, these tokens are specific to each of NASA's 12 different DAACs (despite all 12 DAACs using us-west-2 to host their data. This gives the DAACs some insight into use statistics that could also be obtained by the S3 logs anyway). Most inconveniently of all, these tokens **expire after one hour** and must be renewed, potentially interrupting precisely the kind of intensive, long-running computations a user would want to be inside the local area network to run. 





```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path="img/",
  message = FALSE,
  warning = FALSE
)
```

```{r setup, message=FALSE}
library(earthdatalogin)
library(rstac)
library(gdalcubes)
gdalcubes_options(parallel = TRUE) 
```


### Earth Data Authentication

First let's get EDL authentication out of the way.
For cloud data from almost any other STAC catalog (NOAA, USGS, Planetary Computer, etc), authentication is either unnecessary or already provided by the STAC API, but NASA EDL is special. 


```{r}
library(earthdatalogin)
```

We could just use `edl_set_token()` here as usual to set the environmental variable. 
This works fine but can problems if we do not remember to `edl_unset_token()` before accessing other non-EDL resources over the http interface.
When using the `gdalcubes` package, we have support for a somewhat nicer, more localized authentication that uses the configuration options instead.
We tell `edl_set_token` not to set the environmental variable globally, but to return
in the header format which we can pass to `gdalcubes_set_gdal_config()`:

```{r}
#header <- edl_set_token(set_env_var = FALSE, format = "header")
#gdalcubes_set_gdal_config("GDAL_HTTP_HEADERS", header)
```

`earthdatalogin` also includes optional configuration settings for GDAL which can improve performance of cloud-based data access.  Set the GDAL environmental variables using `gdal_cloud_config()`

```{r}
gdal_cloud_config()
```

## Search via STAC

We will now use the `rstac` package to search one or more NASA collections for data that falls into our desired bounding 

Set a search box in space & time

```{r}
bbox <- c(xmin=-122.5, ymin=37.5, xmax=-122.0, ymax=38) 
start <- "2022-01-01"
end <- "2022-06-30"

# Find all assets from the desired catalog:
items <- stac("https://cmr.earthdata.nasa.gov/stac/LPCLOUD") |> 
  stac_search(collections = "HLSL30.v2.0",
              bbox = bbox,
              datetime = paste(start,end, sep = "/")) |>
  post_request() |>
  items_fetch() |>
  items_filter(filter_fn = \(x) {x[["eo:cloud_cover"]] < 20})
```


Note that `r length(items$features)` features have matched our search criteria! Each feature represents a 'snapshot' image taken by the satellite as it passes by (this is a harmonized product so actually there's quite a lot of post-processing.)  Each feature thus shares the same bounding box, projection, and timestamp, but may consist of many different 'assets', different COG files representing the different spectral bands on the satellite camera instrument.  Each feature can potentially include quite extensive metadata about the feature, including details of instrument itself or from post-processing, such as cloud cover.  Unfortunately, EarthData's STAC metadata tends to be quite sparse. 



## Building a Data Cube


```{r}
# Desired data cube shape & resolution
v = cube_view(srs = "EPSG:4326",
              extent = list(t0 = as.character(start), 
                            t1 = as.character(end),
                            left = bbox[1], right = bbox[3],
                            top = bbox[4], bottom = bbox[2]),
                nx = 2048, ny = 2048, dt = "P1M")
```


```{r}
# RGB bands + cloud cover mask
edl_s3_token()

s3url <- function(url) edl_as_s3(url, prefix="/vsis3/")
col <- stac_image_collection(items$features, 
                             asset_names = c("B02", "B03", "B04", "Fmask"),
                             url_fun = s3url)
```


```{r sf_bay}
# use a cloud mask -- not sure I have this correct
# https://lpdaac.usgs.gov/documents/1326/HLS_User_Guide_V2.pdf
cloud_mask <- image_mask("Fmask", values=1) # mask clouds and cloud shadows
rgb_bands <- c("B04","B03", "B02")

# Here we go! note eval is lazy
raster_cube(col, v, mask=cloud_mask) |>
  select_bands(rgb_bands) |>
  plot(rgb=1:3)
```

